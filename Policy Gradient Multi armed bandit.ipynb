{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient based Multi-armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bandits = [0.2, 0, -0.2, -5] #The lower the bandit number the more likely a positive reward will be returned.\n",
    "num_bandits = len(bandits)\n",
    "\n",
    "def pull_bandit(bandit):\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        #return positive reward.\n",
    "        return 1\n",
    "    else:\n",
    "        #return negative reward.\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#weights are defined for the feed forward network.\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "#action is choosent based on greedy approach.\n",
    "choosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight) * reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Agent :\n",
    "\n",
    "Our Agent is trained by taking actions in our environment, and receiveing rewards. Using the rewards and actions, we can know how to properly update our network in order to more often choose actions that will yield the highest rewards over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoreboard  :  [-1.  0.  0.  0.]\n",
      "Scoreboard  :  [ -4.  -1.   0.  28.]\n",
      "Scoreboard  :  [ -5.  -3.   2.  69.]\n",
      "Scoreboard  :  [  -4.   -1.    2.  108.]\n",
      "Scoreboard  :  [  -2.   -1.    0.  150.]\n",
      "Scoreboard  :  [  -2.    0.    0.  191.]\n",
      "Scoreboard  :  [  -1.    3.    0.  233.]\n",
      "Scoreboard  :  [  -2.    9.   -1.  273.]\n",
      "Scoreboard  :  [  -2.   10.   -1.  318.]\n",
      "Scoreboard  :  [  -5.   10.    0.  362.]\n",
      "Scoreboard  :  [  -1.   12.    1.  405.]\n",
      "Scoreboard  :  [   0.   15.    1.  449.]\n",
      "Scoreboard  :  [   0.   16.    1.  490.]\n",
      "Scoreboard  :  [  -1.   17.   -1.  530.]\n",
      "Scoreboard  :  [   0.   18.   -2.  569.]\n",
      "Scoreboard  :  [   0.   14.   -3.  614.]\n",
      "Scoreboard  :  [   0.   14.   -4.  657.]\n",
      "Scoreboard  :  [   0.   13.   -3.  703.]\n",
      "Scoreboard  :  [   0.   13.    0.  744.]\n",
      "Scoreboard  :  [   0.   11.    2.  780.]\n",
      "Our trained agent believes the most promising agent is : 4\n",
      "Yes! your agent is good.\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000 #This parameter sets how many times we are going to play the game.\n",
    "total_reward = np.zeros(num_bandits) #Initializing the scoreboard of bandits to zero.\n",
    "e = 0.2 #This parameter sets the chance of taking random action.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        \n",
    "        #Choose either a random action or from our network.\n",
    "        if(np.random.rand(1) < e):\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            action = sess.run(choosen_action)\n",
    "            \n",
    "        #On choosing an action by picking one of the bandits we get a reward.\n",
    "        reward = pull_bandit(bandits[action])\n",
    "        \n",
    "        #Updating the weights in the network\n",
    "        _, resp, ww = sess.run([update, responsible_weight, weights], feed_dict={reward_holder: [reward], action_holder: [action]})\n",
    "        \n",
    "        #Updating our scoreboard\n",
    "        total_reward[action] += reward\n",
    "        if(i%50 == 0):\n",
    "            print(\"Scoreboard  :  \" + str(total_reward))\n",
    "        i += 1\n",
    "\n",
    "print(\"Our trained agent believes the most promising agent is : \" + str(np.argmax(ww) + 1))\n",
    "if(np.argmax(ww) == np.argmax(-np.array(bandits))):\n",
    "    print(\"Yes! your agent is good.\")\n",
    "else:\n",
    "    print(\"Sorry, better train you agent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
